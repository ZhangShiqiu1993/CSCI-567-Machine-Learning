
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{main}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \section{Programming Homework 1}\label{programming-homework-1}

\subsection{Instructions}\label{instructions}

\begin{itemize}
\tightlist
\item
  Do not import other libraries. You are only allowed to use Math,
  Numpy, Scipy packages which are already imported in the file.
\item
  Please follow the type annotations. There are some type annotations of
  the parameters of function calls and return values. Please use Python
  3.5 or 3.6 (for full support for typing annotations). You can use
  Numpy/Scipy inside the function. You have to make the functions'
  return values converted to the required type.
\item
  In this programming assignment you will to implement \textbf{Linear
  Regression}, **K-Nearest Neighbours* and *Perceptron
  algorithm\textbf{. We provide the bootstrap code and you are expected
  to complete the }classes\textbf{ and }functions**.
\end{itemize}

    \section{Problem 1: Linear
Regression}\label{problem-1-linear-regression}

    \subsection{Part 1.1 Implementation}\label{part-1.1-implementation}

    \textbf{Implement} the classes in file \emph{hw1\_lr.py}.

\begin{verbatim}
- LinearRegression
- LinearRegressionWithL2Loss
\end{verbatim}

and the function in file \emph{utils.py}:

\begin{verbatim}
- mean_squared_error
\end{verbatim}

For linear regression with l2 loss (a.k.a. Ridge loss), here are two
useful links:

\begin{itemize}
\tightlist
\item
  \href{https://goo.gl/iTX39z}{a tutorial blog}
\item
  \href{http://www.stat.cmu.edu/~ryantibs/datamining/lectures/16-modr1.pdf}{a
  lecture slide}.
\end{itemize}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}1}]:} \PY{c+c1}{\PYZsh{} for auto\PYZhy{}reloading external modules}
        \PY{c+c1}{\PYZsh{} see http://stackoverflow.com/questions/1907993/autoreload\PYZhy{}of\PYZhy{}modules\PYZhy{}in\PYZhy{}ipython}
        \PY{o}{\PYZpc{}}\PY{k}{load\PYZus{}ext} autoreload
        \PY{o}{\PYZpc{}}\PY{k}{autoreload} 2
        \PY{k+kn}{from} \PY{n+nn}{hw1\PYZus{}lr} \PY{k}{import} \PY{n}{LinearRegression}\PY{p}{,} \PY{n}{LinearRegressionWithL2Loss}
        \PY{k+kn}{from} \PY{n+nn}{utils} \PY{k}{import} \PY{n}{mean\PYZus{}squared\PYZus{}error}
        
        
        \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
\end{Verbatim}


    \subsection{Part 1.2 Sanity test}\label{part-1.2-sanity-test}

Do the following steps, as a simple test to check your model works
correctly.

\begin{itemize}
\tightlist
\item
  Load data (features and values) from function
  \texttt{generate\_data\_part\_1}.
\item
  Create a LinearRegression model.
\item
  Train the model using the loaded data.
\item
  Calculate the MSE metric by your implementation of the mean squared
  error function.
\end{itemize}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}2}]:} \PY{k+kn}{from} \PY{n+nn}{data} \PY{k}{import} \PY{n}{generate\PYZus{}data\PYZus{}part\PYZus{}1}
        \PY{n}{features}\PY{p}{,} \PY{n}{values} \PY{o}{=} \PY{n}{generate\PYZus{}data\PYZus{}part\PYZus{}1}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3}]:} \PY{n}{model} \PY{o}{=} \PY{n}{LinearRegression}\PY{p}{(}\PY{n}{nb\PYZus{}features}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
        \PY{n}{model}\PY{o}{.}\PY{n}{train}\PY{p}{(}\PY{n}{features}\PY{p}{,} \PY{n}{values}\PY{p}{)}
        
        \PY{n}{mse} \PY{o}{=} \PY{n}{mean\PYZus{}squared\PYZus{}error}\PY{p}{(}\PY{n}{values}\PY{p}{,} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{features}\PY{p}{)}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{[part 1.2]}\PY{l+s+se}{\PYZbs{}t}\PY{l+s+s1}{mse: }\PY{l+s+si}{\PYZob{}mse:.5f\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{mse}\PY{o}{=}\PY{n}{mse}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
[part 1.2]	mse: 0.00175

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4}]:} \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{p}{[}\PY{n}{x}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{k}{for} \PY{n}{x} \PY{o+ow}{in} \PY{n}{features}\PY{p}{]}\PY{p}{,} \PY{n}{values}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{origin}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{;}
        \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{p}{[}\PY{n}{x}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{k}{for} \PY{n}{x} \PY{o+ow}{in} \PY{n}{features}\PY{p}{]}\PY{p}{,} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{features}\PY{p}{)}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{predicted}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{;}
        \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}4}]:} <matplotlib.legend.Legend at 0x119957f28>
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_8_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsection{Part 1.3 Feature
Engineering}\label{part-1.3-feature-engineering}

In this part, we are following the same procedure as that in part

a). First we will try the same process as above. We will try two cases :
- No extra features - Adding polynomial features

    \subsubsection{Part 1.3.1 No extra
features}\label{part-1.3.1-no-extra-features}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{k+kn}{from} \PY{n+nn}{data} \PY{k}{import} \PY{n}{generate\PYZus{}data\PYZus{}part\PYZus{}2}
        \PY{n}{features}\PY{p}{,} \PY{n}{values} \PY{o}{=} \PY{n}{generate\PYZus{}data\PYZus{}part\PYZus{}2}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{n}{model} \PY{o}{=} \PY{n}{LinearRegression}\PY{p}{(}\PY{n}{nb\PYZus{}features}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
        \PY{n}{model}\PY{o}{.}\PY{n}{train}\PY{p}{(}\PY{n}{features}\PY{p}{,} \PY{n}{values}\PY{p}{)}
        
        \PY{n}{mse} \PY{o}{=} \PY{n}{mean\PYZus{}squared\PYZus{}error}\PY{p}{(}\PY{n}{values}\PY{p}{,} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{features}\PY{p}{)}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{[part 1.3.1]}\PY{l+s+se}{\PYZbs{}t}\PY{l+s+s1}{mse: }\PY{l+s+si}{\PYZob{}mse:.5f\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{mse}\PY{o}{=}\PY{n}{mse}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
[part 1.3.1]	mse: 0.39997

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}7}]:} \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{p}{[}\PY{n}{x}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{k}{for} \PY{n}{x} \PY{o+ow}{in} \PY{n}{features}\PY{p}{]}\PY{p}{,} \PY{n}{values}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{origin}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{;}
        \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{p}{[}\PY{n}{x}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{k}{for} \PY{n}{x} \PY{o+ow}{in} \PY{n}{features}\PY{p}{]}\PY{p}{,} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{features}\PY{p}{)}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{predicted}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{;}
        \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}7}]:} <matplotlib.legend.Legend at 0x119a76320>
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_13_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsubsection{Part 1.3.2 Adding polynomial
features}\label{part-1.3.2-adding-polynomial-features}

Note that in that synthetic dataset, the value is actually not linear
with the feature (it is exponential). Inspired by Taylor expansion, we
are going to add some polynomial features based on existing features.
More specifically, support \[{\vec {x}} = [x_1, ..., x_n]\] is the
feature vector of one sample (whose value is \(y\)). Instead of modeling
the relationship between \(\vec{x}\) and \(y\), we modeling the
relationship of \(y\) and \(\vec{x'}\), where (suppose we are adding up
to \(k\)-th degree polynomials)

\[{\vec {x'}}_k = [x_1 , x_2 , ..., x_n , x_{21} , ..., x_{2n}, ..., x_{k1}, ..., x_{kn} ]. \]

Repeat the 5 steps, then report the MSE value on training set and model
weights for the following three cases: \(k = 2, k = 4, k = 8\) (for all
numbers, keep 6 digits after the decimal point).

\subsubsection{!!! Make sure that features's polynomials follow the
order in the above
equation.}\label{make-sure-that-featuress-polynomials-follow-the-order-in-the-above-equation.}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}8}]:} \PY{k+kn}{from} \PY{n+nn}{utils} \PY{k}{import} \PY{n}{polynomial\PYZus{}features}
        \PY{n}{features}\PY{p}{,} \PY{n}{values} \PY{o}{=} \PY{n}{generate\PYZus{}data\PYZus{}part\PYZus{}2}\PY{p}{(}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{p}{[}\PY{n}{x}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{k}{for} \PY{n}{x} \PY{o+ow}{in} \PY{n}{features}\PY{p}{]}\PY{p}{,} \PY{n}{values}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{origin}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{;}
        
        \PY{k}{for} \PY{n}{k} \PY{o+ow}{in} \PY{p}{[}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{8}\PY{p}{]}\PY{p}{:}
            \PY{n}{features\PYZus{}extended} \PY{o}{=} \PY{n}{polynomial\PYZus{}features}\PY{p}{(}\PY{n}{features}\PY{p}{,} \PY{n}{k}\PY{p}{)}
            \PY{n}{model} \PY{o}{=} \PY{n}{LinearRegression}\PY{p}{(}\PY{n}{nb\PYZus{}features}\PY{o}{=}\PY{n}{k}\PY{p}{)}
            \PY{n}{model}\PY{o}{.}\PY{n}{train}\PY{p}{(}\PY{n}{features\PYZus{}extended}\PY{p}{,} \PY{n}{values}\PY{p}{)}
            \PY{n}{mse} \PY{o}{=} \PY{n}{mean\PYZus{}squared\PYZus{}error}\PY{p}{(}\PY{n}{values}\PY{p}{,} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{features\PYZus{}extended}\PY{p}{)}\PY{p}{)}
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{[part 1.3.2]}\PY{l+s+se}{\PYZbs{}t}\PY{l+s+s1}{k: }\PY{l+s+si}{\PYZob{}k:d\PYZcb{}}\PY{l+s+se}{\PYZbs{}t}\PY{l+s+s1}{mse: }\PY{l+s+si}{\PYZob{}mse:.5f\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{k}\PY{o}{=}\PY{n}{k}\PY{p}{,} \PY{n}{mse}\PY{o}{=}\PY{n}{mse}\PY{p}{)}\PY{p}{)}
            \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{p}{[}\PY{n}{x}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{k}{for} \PY{n}{x} \PY{o+ow}{in} \PY{n}{features}\PY{p}{]}\PY{p}{,} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{features\PYZus{}extended}\PY{p}{)}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{k=}\PY{l+s+si}{\PYZob{}k\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{k}\PY{o}{=}\PY{n}{k}\PY{p}{)}\PY{p}{)}\PY{p}{;}
        \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
[part 1.3.2]	k: 2	mse: 0.02315
[part 1.3.2]	k: 4	mse: 0.00002
[part 1.3.2]	k: 8	mse: 0.00000

    \end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}8}]:} <matplotlib.legend.Legend at 0x119c19e48>
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_15_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsection{Part 1.4 Train, validation,
test}\label{part-1.4-train-validation-test}

\subsubsection{Data processing}\label{data-processing}

Do the following steps:

\begin{itemize}
\tightlist
\item
  Load data (features and values) from function generate data part 3.
  It's a classification dataset, but we just use it as a regression
  dataset in this assignment.
\item
  Check that there are 150 data samples and each sample have a feature
  vector of length 4.
\item
  Split the whole data set into three parts:

  \begin{itemize}
  \tightlist
  \item
    the train set contains first 100 samples (0th - 99th samples),
  \item
    the validation set contains the next 20 samples (100th - 119th
    samples),
  \item
    the test set contains the rest 30 samples (120th - 149th samples).
  \end{itemize}
\end{itemize}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}9}]:} \PY{k+kn}{from} \PY{n+nn}{data} \PY{k}{import} \PY{n}{generate\PYZus{}data\PYZus{}part\PYZus{}3}
        \PY{n}{features}\PY{p}{,} \PY{n}{values} \PY{o}{=} \PY{n}{generate\PYZus{}data\PYZus{}part\PYZus{}3}\PY{p}{(}\PY{p}{)}
        
        \PY{n}{train\PYZus{}features}\PY{p}{,} \PY{n}{train\PYZus{}values} \PY{o}{=} \PY{n}{features}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{100}\PY{p}{]}\PY{p}{,} \PY{n}{values}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{100}\PY{p}{]}
        \PY{n}{valid\PYZus{}features}\PY{p}{,} \PY{n}{valid\PYZus{}values} \PY{o}{=} \PY{n}{features}\PY{p}{[}\PY{l+m+mi}{100}\PY{p}{:}\PY{l+m+mi}{120}\PY{p}{]}\PY{p}{,} \PY{n}{values}\PY{p}{[}\PY{l+m+mi}{100}\PY{p}{:}\PY{l+m+mi}{120}\PY{p}{]}
        \PY{n}{test\PYZus{}features}\PY{p}{,} \PY{n}{test\PYZus{}values} \PY{o}{=} \PY{n}{features}\PY{p}{[}\PY{l+m+mi}{120}\PY{p}{:}\PY{p}{]}\PY{p}{,} \PY{n}{values}\PY{p}{[}\PY{l+m+mi}{120}\PY{p}{:}\PY{p}{]}
        
        \PY{k}{assert} \PY{n+nb}{len}\PY{p}{(}\PY{n}{train\PYZus{}features}\PY{p}{)} \PY{o}{==} \PY{n+nb}{len}\PY{p}{(}\PY{n}{train\PYZus{}values}\PY{p}{)} \PY{o}{==} \PY{l+m+mi}{100}
        \PY{k}{assert} \PY{n+nb}{len}\PY{p}{(}\PY{n}{valid\PYZus{}features}\PY{p}{)} \PY{o}{==} \PY{n+nb}{len}\PY{p}{(}\PY{n}{valid\PYZus{}values}\PY{p}{)} \PY{o}{==} \PY{l+m+mi}{20}
        \PY{k}{assert} \PY{n+nb}{len}\PY{p}{(}\PY{n}{test\PYZus{}features}\PY{p}{)} \PY{o}{==} \PY{n+nb}{len}\PY{p}{(}\PY{n}{test\PYZus{}values}\PY{p}{)} \PY{o}{==} \PY{l+m+mi}{30}
\end{Verbatim}


    \subsubsection{Part 1.4.1
LinearRegression}\label{part-1.4.1-linearregression}

    \subsubsection{Hyper-parameter and model
selection}\label{hyper-parameter-and-model-selection}

For linear regression model with extra polynomial features, \(k\) is a
hyper-parameter. To choose the best one, we have to

\begin{verbatim}
- train a model with that hyper-parameter based on the train set, 
- calculate its performance on the validation set
- select the best hyper-parameter (the trained model has the best performance on validation set). 
\end{verbatim}

In this task, we only search \(k\) among the set \{1, 3, 10\}.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}10}]:} \PY{n}{best\PYZus{}mse}\PY{p}{,} \PY{n}{best\PYZus{}k} \PY{o}{=} \PY{l+m+mf}{1e10}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}
         \PY{k}{for} \PY{n}{k} \PY{o+ow}{in} \PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{]}\PY{p}{:}
             \PY{n}{train\PYZus{}features\PYZus{}extended} \PY{o}{=} \PY{n}{polynomial\PYZus{}features}\PY{p}{(}\PY{n}{train\PYZus{}features}\PY{p}{,} \PY{n}{k}\PY{p}{)}
         
             \PY{n}{model} \PY{o}{=} \PY{n}{LinearRegression}\PY{p}{(}\PY{n}{nb\PYZus{}features}\PY{o}{=}\PY{n}{k}\PY{p}{)}
             \PY{n}{model}\PY{o}{.}\PY{n}{train}\PY{p}{(}\PY{n}{train\PYZus{}features\PYZus{}extended}\PY{p}{,} \PY{n}{train\PYZus{}values}\PY{p}{)}
             \PY{n}{train\PYZus{}mse} \PY{o}{=} \PY{n}{mean\PYZus{}squared\PYZus{}error}\PY{p}{(}\PY{n}{train\PYZus{}values}\PY{p}{,} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{train\PYZus{}features\PYZus{}extended}\PY{p}{)}\PY{p}{)}
             
             \PY{n}{valid\PYZus{}features\PYZus{}extended} \PY{o}{=} \PY{n}{polynomial\PYZus{}features}\PY{p}{(}\PY{n}{valid\PYZus{}features}\PY{p}{,} \PY{n}{k}\PY{p}{)}
             \PY{n}{valid\PYZus{}mse} \PY{o}{=} \PY{n}{mean\PYZus{}squared\PYZus{}error}\PY{p}{(}\PY{n}{valid\PYZus{}values}\PY{p}{,} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{valid\PYZus{}features\PYZus{}extended}\PY{p}{)}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{[part 1.4.1]}\PY{l+s+se}{\PYZbs{}t}\PY{l+s+s1}{k: }\PY{l+s+si}{\PYZob{}k:d\PYZcb{}}\PY{l+s+se}{\PYZbs{}t}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{k}\PY{o}{=}\PY{n}{k}\PY{p}{)} \PY{o}{+} 
                   \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train mse: }\PY{l+s+si}{\PYZob{}train\PYZus{}mse:.5f\PYZcb{}}\PY{l+s+se}{\PYZbs{}t}\PY{l+s+s1}{valid mse: }\PY{l+s+si}{\PYZob{}valid\PYZus{}mse:.5f\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}
                       \PY{n}{train\PYZus{}mse}\PY{o}{=}\PY{n}{train\PYZus{}mse}\PY{p}{,} \PY{n}{valid\PYZus{}mse}\PY{o}{=}\PY{n}{valid\PYZus{}mse}\PY{p}{)}\PY{p}{)}
         
             \PY{k}{if} \PY{n}{valid\PYZus{}mse} \PY{o}{\PYZlt{}} \PY{n}{best\PYZus{}mse}\PY{p}{:}
                 \PY{n}{best\PYZus{}mse}\PY{p}{,} \PY{n}{best\PYZus{}k} \PY{o}{=} \PY{n}{valid\PYZus{}mse}\PY{p}{,} \PY{n}{k}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
[part 1.4.1]	k: 1	train mse: 0.00909	valid mse: 0.33937
[part 1.4.1]	k: 3	train mse: 0.00261	valid mse: 3.28984
[part 1.4.1]	k: 10	train mse: 0.00002	valid mse: 17435.94431

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}11}]:} \PY{n}{combined\PYZus{}features\PYZus{}extended} \PY{o}{=} \PY{n}{polynomial\PYZus{}features}\PY{p}{(}\PY{n}{train\PYZus{}features} \PY{o}{+} \PY{n}{test\PYZus{}features}\PY{p}{,} \PY{n}{best\PYZus{}k}\PY{p}{)}
         \PY{n}{model} \PY{o}{=} \PY{n}{LinearRegression}\PY{p}{(}\PY{n}{nb\PYZus{}features}\PY{o}{=}\PY{n}{best\PYZus{}k}\PY{p}{)}
         \PY{n}{model}\PY{o}{.}\PY{n}{train}\PY{p}{(}\PY{n}{combined\PYZus{}features\PYZus{}extended}\PY{p}{,} \PY{n}{train\PYZus{}values} \PY{o}{+} \PY{n}{test\PYZus{}values}\PY{p}{)}
         
         \PY{n}{test\PYZus{}features\PYZus{}extended} \PY{o}{=} \PY{n}{polynomial\PYZus{}features}\PY{p}{(}\PY{n}{test\PYZus{}features}\PY{p}{,} \PY{n}{best\PYZus{}k}\PY{p}{)}
         \PY{n}{test\PYZus{}mse} \PY{o}{=} \PY{n}{mean\PYZus{}squared\PYZus{}error}\PY{p}{(}\PY{n}{test\PYZus{}values}\PY{p}{,} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{test\PYZus{}features\PYZus{}extended}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{[part 1.4.1 Linear Regression]}\PY{l+s+se}{\PYZbs{}t}\PY{l+s+s1}{best\PYZus{}k: }\PY{l+s+si}{\PYZob{}best\PYZus{}k:d\PYZcb{}}\PY{l+s+se}{\PYZbs{}t}\PY{l+s+s1}{test mse: }\PY{l+s+si}{\PYZob{}test\PYZus{}mse:.5f\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}
             \PY{n}{best\PYZus{}k}\PY{o}{=}\PY{n}{best\PYZus{}k}\PY{p}{,} \PY{n}{test\PYZus{}mse}\PY{o}{=}\PY{n}{test\PYZus{}mse}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
[part 1.4.1 Linear Regression]	best\_k: 1	test mse: 0.08778

    \end{Verbatim}

    \subsubsection{Part 1.4.2 Linear Regression With L2 Loss (Ridge
Regression)}\label{part-1.4.2-linear-regression-with-l2-loss-ridge-regression}

    For Ridge (linear regression with l2 loss) Regression, we still need to
search \(k\) among \{1, 3, 10\}. \(\alpha\) is also a hyper-parameter,
in this task, we search \(\alpha\) among the set \{0.01, 0.1, 1, 10\}.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}12}]:} \PY{k+kn}{from} \PY{n+nn}{itertools} \PY{k}{import} \PY{n}{product}
         
         \PY{n}{best\PYZus{}mse}\PY{p}{,} \PY{n}{best\PYZus{}k}\PY{p}{,} \PY{n}{best\PYZus{}alpha} \PY{o}{=} \PY{l+m+mf}{1e10}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}
         \PY{k}{for} \PY{n}{k}\PY{p}{,} \PY{n}{alpha} \PY{o+ow}{in} \PY{n}{product}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mf}{0.01}\PY{p}{,} \PY{l+m+mf}{0.1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{]}\PY{p}{)}\PY{p}{:}
             \PY{n}{train\PYZus{}features\PYZus{}extended} \PY{o}{=} \PY{n}{polynomial\PYZus{}features}\PY{p}{(}\PY{n}{train\PYZus{}features}\PY{p}{,} \PY{n}{k}\PY{p}{)}
             \PY{n}{model} \PY{o}{=} \PY{n}{LinearRegressionWithL2Loss}\PY{p}{(}\PY{n}{nb\PYZus{}features}\PY{o}{=}\PY{n}{k}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{n}{alpha}\PY{p}{)}
             \PY{n}{model}\PY{o}{.}\PY{n}{train}\PY{p}{(}\PY{n}{train\PYZus{}features\PYZus{}extended}\PY{p}{,} \PY{n}{train\PYZus{}values}\PY{p}{)}
             \PY{n}{train\PYZus{}mse} \PY{o}{=} \PY{n}{mean\PYZus{}squared\PYZus{}error}\PY{p}{(}\PY{n}{train\PYZus{}values}\PY{p}{,} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{train\PYZus{}features\PYZus{}extended}\PY{p}{)}\PY{p}{)}
         
             \PY{n}{valid\PYZus{}features\PYZus{}extended} \PY{o}{=} \PY{n}{polynomial\PYZus{}features}\PY{p}{(}\PY{n}{valid\PYZus{}features}\PY{p}{,} \PY{n}{k}\PY{p}{)}
             \PY{n}{valid\PYZus{}mse} \PY{o}{=} \PY{n}{mean\PYZus{}squared\PYZus{}error}\PY{p}{(}\PY{n}{valid\PYZus{}values}\PY{p}{,} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{valid\PYZus{}features\PYZus{}extended}\PY{p}{)}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{[part 1.4.2]}\PY{l+s+se}{\PYZbs{}t}\PY{l+s+s1}{k: }\PY{l+s+si}{\PYZob{}k:d\PYZcb{}}\PY{l+s+se}{\PYZbs{}t}\PY{l+s+s1}{alpha: }\PY{l+s+si}{\PYZob{}alpha\PYZcb{}}\PY{l+s+se}{\PYZbs{}t}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{k}\PY{o}{=}\PY{n}{k}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{n}{alpha}\PY{p}{)} \PY{o}{+}
                   \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train mse: }\PY{l+s+si}{\PYZob{}train\PYZus{}mse:.5f\PYZcb{}}\PY{l+s+se}{\PYZbs{}t}\PY{l+s+s1}{valid mse: }\PY{l+s+si}{\PYZob{}valid\PYZus{}mse:.5f\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}
                       \PY{n}{train\PYZus{}mse}\PY{o}{=}\PY{n}{train\PYZus{}mse}\PY{p}{,} \PY{n}{valid\PYZus{}mse}\PY{o}{=}\PY{n}{valid\PYZus{}mse}\PY{p}{)}\PY{p}{)}
             
             \PY{k}{if} \PY{n}{valid\PYZus{}mse} \PY{o}{\PYZlt{}} \PY{n}{best\PYZus{}mse}\PY{p}{:}
                 \PY{n}{best\PYZus{}mse}\PY{p}{,} \PY{n}{best\PYZus{}k}\PY{p}{,} \PY{n}{best\PYZus{}alpha} \PY{o}{=} \PY{n}{valid\PYZus{}mse}\PY{p}{,} \PY{n}{k}\PY{p}{,} \PY{n}{alpha}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
[part 1.4.2]	k: 1	alpha: 0.01	train mse: 0.00909	valid mse: 0.33964
[part 1.4.2]	k: 1	alpha: 0.1	train mse: 0.00911	valid mse: 0.34196
[part 1.4.2]	k: 1	alpha: 1	train mse: 0.00955	valid mse: 0.35604
[part 1.4.2]	k: 1	alpha: 10	train mse: 0.01058	valid mse: 0.40436
[part 1.4.2]	k: 3	alpha: 0.01	train mse: 0.00270	valid mse: 3.33796
[part 1.4.2]	k: 3	alpha: 0.1	train mse: 0.00288	valid mse: 2.69893
[part 1.4.2]	k: 3	alpha: 1	train mse: 0.00336	valid mse: 2.12091
[part 1.4.2]	k: 3	alpha: 10	train mse: 0.00412	valid mse: 1.56060
[part 1.4.2]	k: 10	alpha: 0.01	train mse: 0.00019	valid mse: 15773.72437
[part 1.4.2]	k: 10	alpha: 0.1	train mse: 0.00027	valid mse: 9165.65656
[part 1.4.2]	k: 10	alpha: 1	train mse: 0.00038	valid mse: 2407.71224
[part 1.4.2]	k: 10	alpha: 10	train mse: 0.00084	valid mse: 1519.90973

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}13}]:} \PY{n}{combined\PYZus{}features\PYZus{}extended} \PY{o}{=} \PY{n}{polynomial\PYZus{}features}\PY{p}{(}\PY{n}{train\PYZus{}features} \PY{o}{+} \PY{n}{test\PYZus{}features}\PY{p}{,} \PY{n}{best\PYZus{}k}\PY{p}{)}
         \PY{n}{model} \PY{o}{=} \PY{n}{LinearRegressionWithL2Loss}\PY{p}{(}\PY{n}{nb\PYZus{}features}\PY{o}{=}\PY{n}{best\PYZus{}k}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{n}{best\PYZus{}alpha}\PY{p}{)}
         \PY{n}{model}\PY{o}{.}\PY{n}{train}\PY{p}{(}\PY{n}{combined\PYZus{}features\PYZus{}extended}\PY{p}{,} \PY{n}{train\PYZus{}values} \PY{o}{+} \PY{n}{test\PYZus{}values}\PY{p}{)}
         
         \PY{n}{test\PYZus{}features\PYZus{}extended} \PY{o}{=} \PY{n}{polynomial\PYZus{}features}\PY{p}{(}\PY{n}{test\PYZus{}features}\PY{p}{,} \PY{n}{best\PYZus{}k}\PY{p}{)}
         \PY{n}{test\PYZus{}mse} \PY{o}{=} \PY{n}{mean\PYZus{}squared\PYZus{}error}\PY{p}{(}\PY{n}{test\PYZus{}values}\PY{p}{,} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{test\PYZus{}features\PYZus{}extended}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{[part 1.4.2]}\PY{l+s+se}{\PYZbs{}t}\PY{l+s+s1}{best\PYZus{}k: }\PY{l+s+si}{\PYZob{}best\PYZus{}k:d\PYZcb{}}\PY{l+s+se}{\PYZbs{}t}\PY{l+s+s1}{best\PYZus{}alpha: }\PY{l+s+si}{\PYZob{}best\PYZus{}alpha:f\PYZcb{}}\PY{l+s+se}{\PYZbs{}t}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}
             \PY{n}{best\PYZus{}k}\PY{o}{=}\PY{n}{best\PYZus{}k}\PY{p}{,} \PY{n}{best\PYZus{}alpha}\PY{o}{=}\PY{n}{best\PYZus{}alpha}\PY{p}{)} \PY{o}{+}
               \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test mse: }\PY{l+s+si}{\PYZob{}test\PYZus{}mse:.5f\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{test\PYZus{}mse}\PY{o}{=}\PY{n}{test\PYZus{}mse}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
[part 1.4.2]	best\_k: 1	best\_alpha: 0.010000	test mse: 0.08777

    \end{Verbatim}

    \subsection{Problem 2: K-nearest neighbor (KNN) for binary
classification}\label{problem-2-k-nearest-neighbor-knn-for-binary-classification}

    \paragraph{Some notes}\label{some-notes}

In this task, we will use three distance functions: (we removed the
vector symbol for simplicity)

\begin{itemize}
\tightlist
\item
  Euclidean distance: \[d(x, y) = \sqrt{\langle x - y, x - y \rangle}\]
\item
  Inner product distance: \[d(x, y ) = \langle x, y \rangle\]
\item
  Gaussian kernel distance:
  \[d(x, y ) = \exp({−\frac 12 \sqrt{\langle x - y, x - y \rangle}}) \]
\end{itemize}

F1-score is a important metric for binary classification, as sometimes
the accuracy metric has the false positive (a good example is in MLAPP
book 2.2.3.1 ``Example: medical diagnosis'', Page 29).

    \subsubsection{Part 2.1 Distance
Functions}\label{part-2.1-distance-functions}

Implement the class in file \emph{hw1\_knn.py} - KNN

and the functions in \emph{utils.py}\\
- f1\_score - euclidean\_distance - inner\_product\_distance -
gaussian\_kernel\_distance

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}14}]:} \PY{c+c1}{\PYZsh{} for auto\PYZhy{}reloading external modules}
         \PY{c+c1}{\PYZsh{} see http://stackoverflow.com/questions/1907993/autoreload\PYZhy{}of\PYZhy{}modules\PYZhy{}in\PYZhy{}ipython}
         \PY{o}{\PYZpc{}}\PY{k}{load\PYZus{}ext} autoreload
         \PY{o}{\PYZpc{}}\PY{k}{autoreload} 2
         
         \PY{k+kn}{from} \PY{n+nn}{hw1\PYZus{}knn} \PY{k}{import} \PY{n}{KNN}
         \PY{k+kn}{from} \PY{n+nn}{utils} \PY{k}{import} \PY{n}{euclidean\PYZus{}distance}\PY{p}{,} \PY{n}{gaussian\PYZus{}kernel\PYZus{}distance}\PY{p}{,} \PY{n}{inner\PYZus{}product\PYZus{}distance}
         \PY{k+kn}{from} \PY{n+nn}{utils} \PY{k}{import} \PY{n}{f1\PYZus{}score}
         
         \PY{n}{distance\PYZus{}funcs} \PY{o}{=} \PY{p}{\PYZob{}}
             \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{euclidean}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{euclidean\PYZus{}distance}\PY{p}{,}
             \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{gaussian}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{gaussian\PYZus{}kernel\PYZus{}distance}\PY{p}{,}
             \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{inner\PYZus{}prod}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{inner\PYZus{}product\PYZus{}distance}\PY{p}{,}
         \PY{p}{\PYZcb{}}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
The autoreload extension is already loaded. To reload it, use:
  \%reload\_ext autoreload

    \end{Verbatim}

    \paragraph{Data processing}\label{data-processing}

Do the following steps:

\begin{itemize}
\tightlist
\item
  Load data (features and values) from function generate data cancer
\item
  Check that there are 569 data samples and each sample have a feature
  vector of length 30.
\item
  Split the whole data set into three parts:

  \begin{itemize}
  \tightlist
  \item
    the train set contains first 400 samples (0th - 399th samples),
  \item
    the validation set contains the next 60 samples (400th - 459th
    samples),
  \item
    the test set contains the rest 109 samples (460th - 568th samples).
  \end{itemize}
\end{itemize}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}15}]:} \PY{k+kn}{from} \PY{n+nn}{data} \PY{k}{import} \PY{n}{generate\PYZus{}data\PYZus{}cancer}
         \PY{n}{features}\PY{p}{,} \PY{n}{labels} \PY{o}{=} \PY{n}{generate\PYZus{}data\PYZus{}cancer}\PY{p}{(}\PY{p}{)}
         
         \PY{n}{train\PYZus{}features}\PY{p}{,} \PY{n}{train\PYZus{}labels} \PY{o}{=} \PY{n}{features}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{400}\PY{p}{]}\PY{p}{,} \PY{n}{labels}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{400}\PY{p}{]}
         \PY{n}{valid\PYZus{}features}\PY{p}{,} \PY{n}{valid\PYZus{}labels} \PY{o}{=} \PY{n}{features}\PY{p}{[}\PY{l+m+mi}{400}\PY{p}{:}\PY{l+m+mi}{460}\PY{p}{]}\PY{p}{,} \PY{n}{labels}\PY{p}{[}\PY{l+m+mi}{400}\PY{p}{:}\PY{l+m+mi}{460}\PY{p}{]}
         \PY{n}{test\PYZus{}features}\PY{p}{,} \PY{n}{test\PYZus{}labels} \PY{o}{=} \PY{n}{features}\PY{p}{[}\PY{l+m+mi}{460}\PY{p}{:}\PY{p}{]}\PY{p}{,} \PY{n}{labels}\PY{p}{[}\PY{l+m+mi}{460}\PY{p}{:}\PY{p}{]}
         
         \PY{k}{assert} \PY{n+nb}{len}\PY{p}{(}\PY{n}{train\PYZus{}features}\PY{p}{)} \PY{o}{==} \PY{n+nb}{len}\PY{p}{(}\PY{n}{train\PYZus{}labels}\PY{p}{)} \PY{o}{==} \PY{l+m+mi}{400}
         \PY{k}{assert} \PY{n+nb}{len}\PY{p}{(}\PY{n}{valid\PYZus{}features}\PY{p}{)} \PY{o}{==} \PY{n+nb}{len}\PY{p}{(}\PY{n}{valid\PYZus{}labels}\PY{p}{)} \PY{o}{==} \PY{l+m+mi}{60}
         \PY{k}{assert} \PY{n+nb}{len}\PY{p}{(}\PY{n}{test\PYZus{}features}\PY{p}{)} \PY{o}{==} \PY{n+nb}{len}\PY{p}{(}\PY{n}{test\PYZus{}labels}\PY{p}{)} \PY{o}{==} \PY{l+m+mi}{109}
\end{Verbatim}


    \paragraph{Model selection}\label{model-selection}

In kNN model, the parameter k is a hyper-parameter. In this task, we
only search k among \{1, 3, 10, 20, 50\}.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}16}]:} \PY{k}{for} \PY{n}{name}\PY{p}{,} \PY{n}{func} \PY{o+ow}{in} \PY{n}{distance\PYZus{}funcs}\PY{o}{.}\PY{n}{items}\PY{p}{(}\PY{p}{)}\PY{p}{:}
             \PY{n}{best\PYZus{}f1\PYZus{}score}\PY{p}{,} \PY{n}{best\PYZus{}k} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{0}
             \PY{k}{for} \PY{n}{k} \PY{o+ow}{in} \PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{20}\PY{p}{,} \PY{l+m+mi}{50}\PY{p}{]}\PY{p}{:}
                 \PY{n}{model} \PY{o}{=} \PY{n}{KNN}\PY{p}{(}\PY{n}{k}\PY{o}{=}\PY{n}{k}\PY{p}{,} \PY{n}{distance\PYZus{}function}\PY{o}{=}\PY{n}{func}\PY{p}{)}
                 \PY{n}{model}\PY{o}{.}\PY{n}{train}\PY{p}{(}\PY{n}{train\PYZus{}features}\PY{p}{,} \PY{n}{train\PYZus{}labels}\PY{p}{)}
                 \PY{n}{train\PYZus{}f1\PYZus{}score} \PY{o}{=} \PY{n}{f1\PYZus{}score}\PY{p}{(}
                     \PY{n}{train\PYZus{}labels}\PY{p}{,} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{train\PYZus{}features}\PY{p}{)}\PY{p}{)}
         
                 \PY{n}{valid\PYZus{}f1\PYZus{}score} \PY{o}{=} \PY{n}{f1\PYZus{}score}\PY{p}{(}
                     \PY{n}{valid\PYZus{}labels}\PY{p}{,} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{valid\PYZus{}features}\PY{p}{)}\PY{p}{)}
                 \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{[part 2.1] }\PY{l+s+si}{\PYZob{}name\PYZcb{}}\PY{l+s+se}{\PYZbs{}t}\PY{l+s+s1}{k: }\PY{l+s+si}{\PYZob{}k:d\PYZcb{}}\PY{l+s+se}{\PYZbs{}t}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{name}\PY{o}{=}\PY{n}{name}\PY{p}{,} \PY{n}{k}\PY{o}{=}\PY{n}{k}\PY{p}{)} \PY{o}{+} 
                       \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train: }\PY{l+s+si}{\PYZob{}train\PYZus{}f1\PYZus{}score:.5f\PYZcb{}}\PY{l+s+se}{\PYZbs{}t}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{train\PYZus{}f1\PYZus{}score}\PY{o}{=}\PY{n}{train\PYZus{}f1\PYZus{}score}\PY{p}{)} \PY{o}{+}
                       \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{valid: }\PY{l+s+si}{\PYZob{}valid\PYZus{}f1\PYZus{}score:.5f\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{valid\PYZus{}f1\PYZus{}score}\PY{o}{=}\PY{n}{valid\PYZus{}f1\PYZus{}score}\PY{p}{)}\PY{p}{)}
                 
                 \PY{k}{if} \PY{n}{valid\PYZus{}f1\PYZus{}score} \PY{o}{\PYZgt{}} \PY{n}{best\PYZus{}f1\PYZus{}score}\PY{p}{:}
                     \PY{n}{best\PYZus{}f1\PYZus{}score}\PY{p}{,} \PY{n}{best\PYZus{}k} \PY{o}{=} \PY{n}{valid\PYZus{}f1\PYZus{}score}\PY{p}{,} \PY{n}{k}
         
             \PY{n}{model} \PY{o}{=} \PY{n}{KNN}\PY{p}{(}\PY{n}{k}\PY{o}{=}\PY{n}{best\PYZus{}k}\PY{p}{,} \PY{n}{distance\PYZus{}function}\PY{o}{=}\PY{n}{func}\PY{p}{)}
             \PY{n}{model}\PY{o}{.}\PY{n}{train}\PY{p}{(}\PY{n}{train\PYZus{}features} \PY{o}{+} \PY{n}{valid\PYZus{}features}\PY{p}{,}
                         \PY{n}{train\PYZus{}labels} \PY{o}{+} \PY{n}{valid\PYZus{}labels}\PY{p}{)}
             \PY{n}{test\PYZus{}f1\PYZus{}score} \PY{o}{=} \PY{n}{f1\PYZus{}score}\PY{p}{(}\PY{n}{test\PYZus{}labels}\PY{p}{,} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{test\PYZus{}features}\PY{p}{)}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{[part 2.1] }\PY{l+s+si}{\PYZob{}name\PYZcb{}}\PY{l+s+se}{\PYZbs{}t}\PY{l+s+s1}{best\PYZus{}k: }\PY{l+s+si}{\PYZob{}best\PYZus{}k:d\PYZcb{}}\PY{l+s+se}{\PYZbs{}t}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{name}\PY{o}{=}\PY{n}{name}\PY{p}{,} \PY{n}{best\PYZus{}k}\PY{o}{=}\PY{n}{best\PYZus{}k}\PY{p}{)} \PY{o}{+}
                   \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test f1 score: }\PY{l+s+si}{\PYZob{}test\PYZus{}f1\PYZus{}score:.5f\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{test\PYZus{}f1\PYZus{}score}\PY{o}{=}\PY{n}{test\PYZus{}f1\PYZus{}score}\PY{p}{)}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
[part 2.1] euclidean	k: 1	train: 0.91577	valid: 0.96774
[part 2.1] euclidean	k: 3	train: 0.95197	valid: 0.97872
[part 2.1] euclidean	k: 10	train: 0.94577	valid: 0.97872
[part 2.1] euclidean	k: 20	train: 0.94624	valid: 0.97872
[part 2.1] euclidean	k: 50	train: 0.91949	valid: 0.96774

[part 2.1] euclidean	best\_k: 3	test f1 score: 0.95000

[part 2.1] gaussian	k: 1	train: 0.91577	valid: 0.96774
[part 2.1] gaussian	k: 3	train: 0.95197	valid: 0.97872
[part 2.1] gaussian	k: 10	train: 0.94577	valid: 0.97872
[part 2.1] gaussian	k: 20	train: 0.94624	valid: 0.97872
[part 2.1] gaussian	k: 50	train: 0.91949	valid: 0.96774

[part 2.1] gaussian	best\_k: 3	test f1 score: 0.95000

[part 2.1] inner\_prod	k: 1	train: 0.72408	valid: 0.87850
[part 2.1] inner\_prod	k: 3	train: 0.72408	valid: 0.87850
[part 2.1] inner\_prod	k: 10	train: 0.72408	valid: 0.87850
[part 2.1] inner\_prod	k: 20	train: 0.72408	valid: 0.87850
[part 2.1] inner\_prod	k: 50	train: 0.72408	valid: 0.87850

[part 2.1] inner\_prod	best\_k: 1	test f1 score: 0.86458


    \end{Verbatim}

    \subsubsection{Part 2.2 Data
transformation}\label{part-2.2-data-transformation}

We are going to add one more step (data transformation) in the data
processing part and see how it works. Sometimes, normalization plays an
important role to make a machine learning model work (check term
``Feature scaling'' in wiki).

Here, we take two different data transformation approaches.

\paragraph{Normalizing the feature
vector}\label{normalizing-the-feature-vector}

This one is simple but some times may work well. Given a feature vector
\(x\), the normalized feature vector is given by

\[ x' = \frac x {\sqrt{\langle x, x \rangle}} \] If a vector is a
all-zero vector, we let the normalized vector also be a all-zero vector.

\paragraph{Min-max scaling the feature
matrix}\label{min-max-scaling-the-feature-matrix}

The above normalization is data independent, that is to say, the output
of the normalization function doesn't depend on the rest training data.
However, sometimes it would be helpful to do data dependent
normalization. One thing to note is that, when doing data dependent
normalization, we can only use training data, as the test data is
assumed to be unknown during training (at least for most classification
tasks).

The min-max scaling works as follows: after min-max scaling, all values
of training data's feature vectors are in the given range. Note that
this doesn't mean the values of the validation/test data's fea- tures
are all in that range, because the validation/test data may have dif-
ferent distribution as the training data.

    \textbf{Implement} the functions in \emph{utils.py}\\
- normalize - min\_max\_scale

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}17}]:} \PY{k+kn}{from} \PY{n+nn}{utils} \PY{k}{import} \PY{n}{normalize}\PY{p}{,} \PY{n}{min\PYZus{}max\PYZus{}scale}
         
         \PY{n}{scaling\PYZus{}functions} \PY{o}{=} \PY{p}{\PYZob{}}
             \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{min\PYZus{}max\PYZus{}scale}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{min\PYZus{}max\PYZus{}scale}\PY{p}{,}
             \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{normalize}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{normalize}\PY{p}{,}
         \PY{p}{\PYZcb{}}
\end{Verbatim}


    \paragraph{Model selection}\label{model-selection}

Repeat the model selection part in part 2.2.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}18}]:} \PY{k}{for} \PY{n}{scaling\PYZus{}name}\PY{p}{,} \PY{n}{scaling\PYZus{}func} \PY{o+ow}{in} \PY{n}{scaling\PYZus{}functions}\PY{o}{.}\PY{n}{items}\PY{p}{(}\PY{p}{)}\PY{p}{:}
             \PY{n}{train\PYZus{}features} \PY{o}{=} \PY{n}{scaling\PYZus{}func}\PY{p}{(}\PY{n}{train\PYZus{}features}\PY{p}{)}
             \PY{n}{valid\PYZus{}features} \PY{o}{=} \PY{n}{scaling\PYZus{}func}\PY{p}{(}\PY{n}{valid\PYZus{}features}\PY{p}{)}
             \PY{n}{test\PYZus{}features} \PY{o}{=} \PY{n}{scaling\PYZus{}func}\PY{p}{(}\PY{n}{test\PYZus{}features}\PY{p}{)}
             
             \PY{k}{for} \PY{n}{name}\PY{p}{,} \PY{n}{func} \PY{o+ow}{in} \PY{n}{distance\PYZus{}funcs}\PY{o}{.}\PY{n}{items}\PY{p}{(}\PY{p}{)}\PY{p}{:}
                 \PY{n}{best\PYZus{}f1\PYZus{}score}\PY{p}{,} \PY{n}{best\PYZus{}k} \PY{o}{=} \PY{l+m+mi}{0}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}
                 \PY{k}{for} \PY{n}{k} \PY{o+ow}{in} \PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{20}\PY{p}{,} \PY{l+m+mi}{50}\PY{p}{]}\PY{p}{:}
                     \PY{n}{model} \PY{o}{=} \PY{n}{KNN}\PY{p}{(}\PY{n}{k}\PY{o}{=}\PY{n}{k}\PY{p}{,} \PY{n}{distance\PYZus{}function}\PY{o}{=}\PY{n}{func}\PY{p}{)}
                     \PY{n}{model}\PY{o}{.}\PY{n}{train}\PY{p}{(}\PY{n}{train\PYZus{}features}\PY{p}{,} \PY{n}{train\PYZus{}labels}\PY{p}{)}
                     \PY{n}{train\PYZus{}f1\PYZus{}score} \PY{o}{=} \PY{n}{f1\PYZus{}score}\PY{p}{(}
                         \PY{n}{train\PYZus{}labels}\PY{p}{,} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{train\PYZus{}features}\PY{p}{)}\PY{p}{)}
                     
                     \PY{n}{valid\PYZus{}f1\PYZus{}score} \PY{o}{=} \PY{n}{f1\PYZus{}score}\PY{p}{(}
                         \PY{n}{valid\PYZus{}labels}\PY{p}{,} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{valid\PYZus{}features}\PY{p}{)}\PY{p}{)}
                     \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{[part 2.2] }\PY{l+s+si}{\PYZob{}name\PYZcb{}}\PY{l+s+se}{\PYZbs{}t}\PY{l+s+si}{\PYZob{}scaling\PYZus{}name\PYZcb{}}\PY{l+s+se}{\PYZbs{}t}\PY{l+s+s1}{k: }\PY{l+s+si}{\PYZob{}k:d\PYZcb{}}\PY{l+s+se}{\PYZbs{}t}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{name}\PY{o}{=}\PY{n}{name}\PY{p}{,} \PY{n}{scaling\PYZus{}name}\PY{o}{=}\PY{n}{scaling\PYZus{}name}\PY{p}{,} \PY{n}{k}\PY{o}{=}\PY{n}{k}\PY{p}{)} \PY{o}{+}
                           \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train: }\PY{l+s+si}{\PYZob{}train\PYZus{}f1\PYZus{}score:.5f\PYZcb{}}\PY{l+s+se}{\PYZbs{}t}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{train\PYZus{}f1\PYZus{}score}\PY{o}{=}\PY{n}{train\PYZus{}f1\PYZus{}score}\PY{p}{)} \PY{o}{+} 
                           \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{valid: }\PY{l+s+si}{\PYZob{}valid\PYZus{}f1\PYZus{}score:.5f\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{valid\PYZus{}f1\PYZus{}score}\PY{o}{=}\PY{n}{valid\PYZus{}f1\PYZus{}score}\PY{p}{)}\PY{p}{)}
         
                     \PY{k}{if} \PY{n}{valid\PYZus{}f1\PYZus{}score} \PY{o}{\PYZgt{}} \PY{n}{best\PYZus{}f1\PYZus{}score}\PY{p}{:}
                         \PY{n}{best\PYZus{}f1\PYZus{}score}\PY{p}{,} \PY{n}{best\PYZus{}k} \PY{o}{=} \PY{n}{valid\PYZus{}f1\PYZus{}score}\PY{p}{,} \PY{n}{k}
             
                 \PY{n}{model} \PY{o}{=} \PY{n}{KNN}\PY{p}{(}\PY{n}{k}\PY{o}{=}\PY{n}{best\PYZus{}k}\PY{p}{,} \PY{n}{distance\PYZus{}function}\PY{o}{=}\PY{n}{func}\PY{p}{)}
                 \PY{n}{model}\PY{o}{.}\PY{n}{train}\PY{p}{(}\PY{n}{train\PYZus{}features} \PY{o}{+} \PY{n}{valid\PYZus{}features}\PY{p}{,}
                             \PY{n}{train\PYZus{}labels} \PY{o}{+} \PY{n}{valid\PYZus{}labels}\PY{p}{)}
                 \PY{n}{test\PYZus{}f1\PYZus{}score} \PY{o}{=} \PY{n}{f1\PYZus{}score}\PY{p}{(}\PY{n}{test\PYZus{}labels}\PY{p}{,} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{test\PYZus{}features}\PY{p}{)}\PY{p}{)}
                 \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
                 \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{[part 2.2] }\PY{l+s+si}{\PYZob{}name\PYZcb{}}\PY{l+s+se}{\PYZbs{}t}\PY{l+s+si}{\PYZob{}scaling\PYZus{}name\PYZcb{}}\PY{l+s+se}{\PYZbs{}t}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{name}\PY{o}{=}\PY{n}{name}\PY{p}{,} \PY{n}{scaling\PYZus{}name}\PY{o}{=}\PY{n}{scaling\PYZus{}name}\PY{p}{)} \PY{o}{+}
                       \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{best\PYZus{}k: }\PY{l+s+si}{\PYZob{}best\PYZus{}k:d\PYZcb{}}\PY{l+s+se}{\PYZbs{}t}\PY{l+s+s1}{test: }\PY{l+s+si}{\PYZob{}test\PYZus{}f1\PYZus{}score:.5f\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{best\PYZus{}k}\PY{o}{=}\PY{n}{best\PYZus{}k}\PY{p}{,} \PY{n}{test\PYZus{}f1\PYZus{}score}\PY{o}{=}\PY{n}{test\PYZus{}f1\PYZus{}score}\PY{p}{)}\PY{p}{)}
                 \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
[part 2.2] euclidean	min\_max\_scale	k: 1	train: 0.95595	valid: 0.90698
[part 2.2] euclidean	min\_max\_scale	k: 3	train: 0.97582	valid: 0.91954
[part 2.2] euclidean	min\_max\_scale	k: 10	train: 0.97368	valid: 0.93182
[part 2.2] euclidean	min\_max\_scale	k: 20	train: 0.97155	valid: 0.94382
[part 2.2] euclidean	min\_max\_scale	k: 50	train: 0.95299	valid: 0.94382

[part 2.2] euclidean	min\_max\_scale	best\_k: 20	test: 0.97561

[part 2.2] gaussian	min\_max\_scale	k: 1	train: 0.95595	valid: 0.90698
[part 2.2] gaussian	min\_max\_scale	k: 3	train: 0.97582	valid: 0.91954
[part 2.2] gaussian	min\_max\_scale	k: 10	train: 0.97368	valid: 0.93182
[part 2.2] gaussian	min\_max\_scale	k: 20	train: 0.97155	valid: 0.94382
[part 2.2] gaussian	min\_max\_scale	k: 50	train: 0.95299	valid: 0.94382

[part 2.2] gaussian	min\_max\_scale	best\_k: 20	test: 0.97561

[part 2.2] inner\_prod	min\_max\_scale	k: 1	train: 0.72408	valid: 0.87850
[part 2.2] inner\_prod	min\_max\_scale	k: 3	train: 0.72408	valid: 0.87850
[part 2.2] inner\_prod	min\_max\_scale	k: 10	train: 0.72408	valid: 0.87850
[part 2.2] inner\_prod	min\_max\_scale	k: 20	train: 0.72408	valid: 0.87850
[part 2.2] inner\_prod	min\_max\_scale	k: 50	train: 0.72408	valid: 0.87850

[part 2.2] inner\_prod	min\_max\_scale	best\_k: 1	test: 0.86458

[part 2.2] euclidean	normalize	k: 1	train: 0.89087	valid: 0.96907
[part 2.2] euclidean	normalize	k: 3	train: 0.92694	valid: 0.95833
[part 2.2] euclidean	normalize	k: 10	train: 0.93578	valid: 0.95745
[part 2.2] euclidean	normalize	k: 20	train: 0.91628	valid: 0.95652
[part 2.2] euclidean	normalize	k: 50	train: 0.90487	valid: 0.94505

[part 2.2] euclidean	normalize	best\_k: 1	test: 0.89286

[part 2.2] gaussian	normalize	k: 1	train: 0.89087	valid: 0.96907
[part 2.2] gaussian	normalize	k: 3	train: 0.92694	valid: 0.95833
[part 2.2] gaussian	normalize	k: 10	train: 0.93578	valid: 0.95745
[part 2.2] gaussian	normalize	k: 20	train: 0.91628	valid: 0.95652
[part 2.2] gaussian	normalize	k: 50	train: 0.90487	valid: 0.94505

[part 2.2] gaussian	normalize	best\_k: 1	test: 0.89286

[part 2.2] inner\_prod	normalize	k: 1	train: 0.60177	valid: 0.66667
[part 2.2] inner\_prod	normalize	k: 3	train: 0.65411	valid: 0.66667
[part 2.2] inner\_prod	normalize	k: 10	train: 0.60245	valid: 0.60465
[part 2.2] inner\_prod	normalize	k: 20	train: 0.58214	valid: 0.51852
[part 2.2] inner\_prod	normalize	k: 50	train: 0.51866	valid: 0.48101

[part 2.2] inner\_prod	normalize	best\_k: 1	test: 0.73988


    \end{Verbatim}

    \subsection{Problem 3: Perceptron
Problem}\label{problem-3-perceptron-problem}

In this problem we will implement perceptron algorithm. Recall that
perceptron algorithm can converge only when the data is linearly
seperable.

\subsubsection{Objective}\label{objective}

Implement the class \texttt{Perceptron} in file
\texttt{hw1\_perceptron.py}.

\paragraph{Some notes}\label{some-notes}

\begin{itemize}
\tightlist
\item
  Perceptron update rule is whenever algorithm makes a mistake update
  weights as \[w ← w + \frac{y_i x_i}{\|x\|}\]
\item
  Perceptron algorithm as discussed only works for linearly seperable
  data. In this problem you will see that it is indeed the case.
\item
  For data which is not linearly seperable there is class of
  model/classifiers called maximum margin classifiers which will be
  discussed later.
\end{itemize}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}19}]:} \PY{c+c1}{\PYZsh{} for auto\PYZhy{}reloading external modules}
         \PY{c+c1}{\PYZsh{} see http://stackoverflow.com/questions/1907993/autoreload\PYZhy{}of\PYZhy{}modules\PYZhy{}in\PYZhy{}ipython}
         \PY{o}{\PYZpc{}}\PY{k}{load\PYZus{}ext} autoreload
         \PY{o}{\PYZpc{}}\PY{k}{autoreload} 2
         
         \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt} 
         \PY{k+kn}{from} \PY{n+nn}{data} \PY{k}{import} \PY{n}{generate\PYZus{}data\PYZus{}perceptron} 
         \PY{k+kn}{from} \PY{n+nn}{hw1\PYZus{}perceptron} \PY{k}{import} \PY{n}{Perceptron}
         \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
         
         \PY{c+c1}{\PYZsh{}\PYZsh{} To clearly visualize the problem, we just use 2 features for now}
         \PY{c+c1}{\PYZsh{}\PYZsh{} y = f(x1,x2)}
         
         \PY{n}{nb\PYZus{}features}\PY{o}{=}\PY{l+m+mi}{2}
         \PY{n}{model} \PY{o}{=} \PY{n}{Perceptron}\PY{p}{(}\PY{n}{nb\PYZus{}features}\PY{o}{=}\PY{n}{nb\PYZus{}features}\PY{p}{)}
         \PY{n}{x}\PY{p}{,} \PY{n}{y} \PY{o}{=} \PY{n}{generate\PYZus{}data\PYZus{}perceptron}\PY{p}{(}\PY{n}{nb\PYZus{}features}\PY{o}{=}\PY{n}{nb\PYZus{}features}\PY{p}{,} \PY{n}{seperation}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
The autoreload extension is already loaded. To reload it, use:
  \%reload\_ext autoreload

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}20}]:} \PY{c+c1}{\PYZsh{} plot and satisfy your self that data is linearly seperable}
         \PY{n}{x1} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         \PY{n}{x2} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{x}\PY{p}{:}
             \PY{n}{x1}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{i}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
             \PY{n}{x2}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{i}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{x1}\PY{p}{,} \PY{n}{x2}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{n}{y}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_41_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Now complete the train \& predict function in Perceptron class. You
algorithm should find the seperating hyperplane and model.predict should
give all the labels correct.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}21}]:} \PY{n}{converged} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{train}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{)}
         \PY{n}{y\PYZus{}hat} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{x}\PY{p}{)} 
         \PY{n}{correct} \PY{o}{=} \PY{l+m+mi}{0} 
         \PY{k}{for} \PY{n}{i}\PY{p}{,} \PY{n}{y\PYZus{}real} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{y}\PY{p}{)}\PY{p}{:}
             \PY{k}{if} \PY{p}{(}\PY{n}{y\PYZus{}hat}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{o}{==}\PY{n}{y\PYZus{}real}\PY{p}{)}\PY{p}{:}
                 \PY{n}{correct} \PY{o}{=} \PY{n}{correct} \PY{o}{+} \PY{l+m+mi}{1}
             
         \PY{n+nb}{print} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Accuracy on training data is }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{correct}\PY{o}{*}\PY{l+m+mi}{100}\PY{o}{/}\PY{n+nb}{len}\PY{p}{(}\PY{n}{y}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print} \PY{p}{(}\PY{n}{correct}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Accuracy on training data is 100.0
100

    \end{Verbatim}

    Plot the data points and seperating hyperplane to see your perceptron
has actually learnt correct seperating plane

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}22}]:} \PY{n}{w} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{get\PYZus{}weights}\PY{p}{(}\PY{p}{)}
         \PY{n}{x1} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         \PY{n}{x2} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{x}\PY{p}{:}
             \PY{n}{x1}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{i}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
             \PY{n}{x2}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{i}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{x1}\PY{p}{,} \PY{n}{x2}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{n}{y}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{p}{(}\PY{n}{w}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{*} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)} \PY{o}{+} \PY{n}{w}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)} \PY{o}{/} \PY{n}{w}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_45_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Now we will try a 80-20 split of data into train and test and see what
happens Try shuffling too. The results will change on shuffling

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}23}]:} \PY{c+c1}{\PYZsh{}\PYZsh{} TODO : Try shuffling the data}
         
         \PY{n}{model}\PY{o}{.}\PY{n}{reset}\PY{p}{(}\PY{p}{)}
         \PY{n+nb}{print} \PY{p}{(}\PY{n}{model}\PY{o}{.}\PY{n}{get\PYZus{}weights}\PY{p}{(}\PY{p}{)}\PY{p}{)}
         \PY{n}{test\PYZus{}x}\PY{p}{,} \PY{n}{train\PYZus{}x} \PY{o}{=} \PY{n}{x}\PY{p}{[}\PY{l+m+mi}{80}\PY{p}{:}\PY{p}{]}\PY{p}{,} \PY{n}{x}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{80}\PY{p}{]}
         \PY{n}{test\PYZus{}y}\PY{p}{,} \PY{n}{train\PYZus{}y} \PY{o}{=} \PY{n}{y}\PY{p}{[}\PY{l+m+mi}{80}\PY{p}{:}\PY{p}{]}\PY{p}{,} \PY{n}{y}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{80}\PY{p}{]}
         \PY{n}{converged} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{train}\PY{p}{(}\PY{n}{train\PYZus{}x}\PY{p}{,} \PY{n}{train\PYZus{}y}\PY{p}{)}
         \PY{n}{y\PYZus{}hat} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{test\PYZus{}x}\PY{p}{)} 
         \PY{n}{correct} \PY{o}{=} \PY{l+m+mi}{0} 
         \PY{k}{for} \PY{n}{i}\PY{p}{,} \PY{n}{y\PYZus{}real} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{test\PYZus{}y}\PY{p}{)}\PY{p}{:}
             \PY{k}{if} \PY{p}{(}\PY{n}{y\PYZus{}hat}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{o}{==}\PY{n}{y\PYZus{}real}\PY{p}{)}\PY{p}{:}
                 \PY{n}{correct} \PY{o}{=} \PY{n}{correct} \PY{o}{+} \PY{l+m+mi}{1}
             
         \PY{n+nb}{print} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Accuracy on testing data is }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{correct}\PY{o}{*}\PY{l+m+mi}{100}\PY{o}{/}\PY{n+nb}{len}\PY{p}{(}\PY{n}{test\PYZus{}y}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
[0, 0, 0]
Accuracy on testing data is 100.0

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}24}]:} \PY{n}{w} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{get\PYZus{}weights}\PY{p}{(}\PY{p}{)}
         \PY{n}{x1} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         \PY{n}{x2} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{x}\PY{p}{:}
             \PY{n}{x1}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{i}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
             \PY{n}{x2}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{i}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{x1}\PY{p}{,} \PY{n}{x2}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{n}{y}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{p}{(}\PY{n}{w}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{*} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)} \PY{o}{+} \PY{n}{w}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)} \PY{o}{/} \PY{n}{w}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_48_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsubsection{What if data is not linearly
seperable}\label{what-if-data-is-not-linearly-seperable}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}25}]:} \PY{n}{nb\PYZus{}features}\PY{o}{=}\PY{l+m+mi}{2}
         \PY{n}{model} \PY{o}{=} \PY{n}{Perceptron}\PY{p}{(}\PY{n}{nb\PYZus{}features}\PY{o}{=}\PY{n}{nb\PYZus{}features}\PY{p}{)}
         \PY{n}{x}\PY{p}{,} \PY{n}{y} \PY{o}{=} \PY{n}{generate\PYZus{}data\PYZus{}perceptron}\PY{p}{(}\PY{n}{nb\PYZus{}features}\PY{o}{=}\PY{n}{nb\PYZus{}features}\PY{p}{,} \PY{n}{seperation}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
         \PY{c+c1}{\PYZsh{} plot and satisfy your self that data is not linearly seperable}
         \PY{n}{x1} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         \PY{n}{x2} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{x}\PY{p}{:}
             \PY{n}{x1}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{i}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
             \PY{n}{x2}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{i}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{x1}\PY{p}{,} \PY{n}{x2}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{n}{y}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_50_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}26}]:} \PY{n}{converged} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{train}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{)}
         \PY{k}{if} \PY{p}{(}\PY{n}{converged}\PY{p}{)}\PY{p}{:}
             \PY{n+nb}{print} \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Algorithm has converged}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{k}{else}\PY{p}{:}
             \PY{n+nb}{print} \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Algorithm didnot converge}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             
         \PY{n}{y\PYZus{}hat} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{x}\PY{p}{)} 
         \PY{n}{correct} \PY{o}{=} \PY{l+m+mi}{0} 
         \PY{k}{for} \PY{n}{i}\PY{p}{,} \PY{n}{y\PYZus{}real} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{y}\PY{p}{)}\PY{p}{:}
             \PY{k}{if} \PY{p}{(}\PY{n}{y\PYZus{}hat}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{o}{==}\PY{n}{y\PYZus{}real}\PY{p}{)}\PY{p}{:}
                 \PY{n}{correct} \PY{o}{=} \PY{n}{correct} \PY{o}{+} \PY{l+m+mi}{1}
             
         \PY{n+nb}{print} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Accuracy on training data is }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{correct}\PY{o}{*}\PY{l+m+mi}{100}\PY{o}{/}\PY{n+nb}{len}\PY{p}{(}\PY{n}{y}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print} \PY{p}{(}\PY{n}{correct}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Algorithm didnot converge
Accuracy on training data is 92.0
92

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}27}]:} \PY{n}{w} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{get\PYZus{}weights}\PY{p}{(}\PY{p}{)}
         \PY{n}{x1} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         \PY{n}{x2} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{x}\PY{p}{:}
             \PY{n}{x1}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{i}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
             \PY{n}{x2}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{i}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{x1}\PY{p}{,} \PY{n}{x2}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{n}{y}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{p}{(}\PY{n}{w}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{*} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)} \PY{o}{+} \PY{n}{w}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)} \PY{o}{/} \PY{n}{w}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_52_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Finding a seperating plane when data is d dimensional. Note that we
visualize only first two features, so data might not look seperable but
might really be seperable

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}28}]:} \PY{n}{nb\PYZus{}features}\PY{o}{=}\PY{l+m+mi}{10}
         \PY{n}{model} \PY{o}{=} \PY{n}{Perceptron}\PY{p}{(}\PY{n}{nb\PYZus{}features}\PY{o}{=}\PY{n}{nb\PYZus{}features}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} use seperation=1 for non\PYZhy{}seperable }
         \PY{c+c1}{\PYZsh{} use seperation=2 for seperable }
         
         \PY{n}{x}\PY{p}{,} \PY{n}{y} \PY{o}{=} \PY{n}{generate\PYZus{}data\PYZus{}perceptron}\PY{p}{(}\PY{n}{nb\PYZus{}features}\PY{o}{=}\PY{n}{nb\PYZus{}features}\PY{p}{,} \PY{n}{seperation}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
         \PY{c+c1}{\PYZsh{} plot first two dimensions}
         \PY{n}{x1} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         \PY{n}{x2} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{x}\PY{p}{:}
             \PY{n}{x1}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{i}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
             \PY{n}{x2}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{i}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{x1}\PY{p}{,} \PY{n}{x2}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{n}{y}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_54_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}29}]:} \PY{n}{converged} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{train}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{)}
         \PY{k}{if} \PY{p}{(}\PY{n}{converged}\PY{p}{)}\PY{p}{:}
             \PY{n+nb}{print} \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Algorithm has converged}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{k}{else}\PY{p}{:}
             \PY{n+nb}{print} \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Algorithm didnot converge}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             
         \PY{n}{y\PYZus{}hat} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{x}\PY{p}{)} 
         \PY{n}{correct} \PY{o}{=} \PY{l+m+mi}{0} 
         \PY{k}{for} \PY{n}{i}\PY{p}{,} \PY{n}{y\PYZus{}real} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{y}\PY{p}{)}\PY{p}{:}
             \PY{k}{if} \PY{p}{(}\PY{n}{y\PYZus{}hat}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{o}{==}\PY{n}{y\PYZus{}real}\PY{p}{)}\PY{p}{:}
                 \PY{n}{correct} \PY{o}{=} \PY{n}{correct} \PY{o}{+} \PY{l+m+mi}{1}
             
         \PY{n+nb}{print} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Accuracy on training data is }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{correct}\PY{o}{*}\PY{l+m+mi}{100}\PY{o}{/}\PY{n+nb}{len}\PY{p}{(}\PY{n}{y}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print} \PY{p}{(}\PY{n}{correct}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Algorithm didnot converge
Accuracy on training data is 81.0
81

    \end{Verbatim}


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
